{\rtf1\ansi\ansicpg1252\uc1\deff0
{\fonttbl{\f0\fmodern\fcharset0\fprq2 CourierNewPSMT;}}
{\colortbl;\red0\green0\blue0;\red255\green255\blue255;}
\paperw12240\paperh15840\margl1800\margr1800\margt1440\margb1440\fet2\ftnbj\aenddoc
\pgnrestart\pgnstarts0
\pard\plain \ltrch\loch {\f0\fs24\b0\i0 A full automation of sIOP in order to deliver access-and-play interoperability is impossible: in order to use the data from nation A, nation B needs their genuine understanding in advance. The major impediments to such automation is laid in the nature of semantics itself: The meaning of a term ultimately relates to what it denotes in reality, whilst this relation cannot be deferred from the shape, structure or other characteristics of the term itself due to its total arbitrariness. This semiotic explanation on semantics is depicted in Figure ##, and confronts us with the inevitable separation between languages (software code, modelling languages and natural languages alike) and entities in reality (e.g., things, events, properties of things). That terms and real entities are fundamentally different is hardly a new insight when dealing with information systems. However, addressing this fundamental distinction is at best extremely academic [@Steels:2008tr] and without practical solutions at all [@Cregan2007]. To overcome the separation between terms and entities is in artificial intelligence (AI) known as the *grounding problem*. And despite the progress of AI, its capability to gain even a beginning of a genuine understanding, also known as "strong AI", does not yet exist and is expected to emerge on the long term only, if ever [@XiuquanLi2017]. Its counterpart "weak AI" with its otherwise highly relevant and important achievements in reasoning, prediction and analysis, is based on machinery that relies on language only and can therefore never make the step to reality on its own [@Scheider:2012tj]. Negligence of the existence of the grounding problem and its semiotic origins gives rise to two major impediments in information technology, as follows:}
\par\plain \f0\fs24\b0\i0
\par\plain {\f0\fs24\b0\i0 * Firstly, we don't understand the characteristics of semantics sufficiently, or in other words, what impact is generated by the grounding problem on the construction of a software agent. If we are asked to point at the semantic parts in a software agent, we can't. While the same question about, e.g., its scalability, will render a lecture about the different principles that are applied and the components that are used to its achievement. Consequently, without clear design principles we are at a loss of how to engineer semantics into software agents, and how to provide for components or artifacts that achieve semantics. }
\par\plain {\f0\fs24\b0\i0 * Secondly, without knowing how to engineer semantics into software, we are lacking the bridgehead within the software agent that connects with the semantic bridge. In other words, we do not even know how a semantic bridge looks like. Is it an integrated version of the data schemata of both agents? Is it an as small as possible *third* scheme that addresses the shared parts of both schemata only, to be connected to each of the other two reduced schemata? Do we leave both schemata intact, and introduce a connection between them instead? Only when we have come to a conclusion on the semantic bridge, we can address subsequent issues that relate to other architectural concerns, such as scalability? }
\par\plain \f0\fs24\b0\i0
\par\plain {\f0\fs24\b0\i0 OMG languages typically have abstract syntax (the metamodel), concrete syntax (notations and diagrams) and semantics (constraints and behavior). The MOF is closed in that it is defined in itself. Moreover, although its semantics provide for constraints and behaviour of its models, it does so about the structural aspects of its instances only. Hence, it lacks a formal way that can be used to judge the truth-value of its instances. Interpreting the (informal) semantics of the language, that is, assigning a truth value to an instance, can therefore not rely on an unambiguous  in different ways, resulting in possibly inconsistent and diverging implementations.}
\par\plain \f0\fs24\b0\i0
\par\plain \f0\fs24\b0\i0
\par\plain \f0\fs24\b0\i0
\par\plain {\f0\fs24\b0\i0 In comparison, scalability was a big architectural concern in the past, requiring custom solutions. In response to this concern, scalability was standardised in the form of architectural patterns, and finally totally embedded and hidden into the infrastructure. We now stand for a similar, but completely different situation. It is similar, because, while sIOP currently requires custom solutions, the ultimate goal is its disappearance into the infrastructure. It is completely different, because unlike scalability we are not addressing a concern over which we have complete control. For the first time in ICT we are entering the realm of reality for which, even after several thousands of years of philosophical debate, no unified view can be given. Does a lake continue *to be a lake*, even when all its water have been vaporised in summer? How many tragic *events* does 9-11 represent, one, three or thousands? In almost all situations, there is no one single semantic truth: In the case of the German steel producer, both semantics on what `time` denotes are equally valid from one\loch\af0\hich\af0\dbch\af0\uc1\u8217\'92s own perspective, and equally less valid from the peer\u8217\'92s perspective. Consequently, lacking the ground for a unified semantics, founding solutions to sIOP on semantic standards may be accepted folklore, it remains a fallacy nonetheless. }
\par\plain \f0\fs24\b0\i0
\par\plain {\f0\fs24\b0\i0 ====}
\par\plain \f0\fs24\b0\i0
\par\plain {\f0\fs24\b0\i0 How to bridge the gap? Three possibilities:}
\par\plain \f0\fs24\b0\i0
\par\plain {\f0\fs24\b0\i0 * Monolithic approach: Agree on the semantics of all terms}
\par\plain {\f0\fs24\b0\i0 * Alignment approach: }
\par\plain \f0\fs24\b0\i0
\par\plain {\f0\fs24\b0\i0 ====}
\par\plain \f0\fs24\b0\i0
\par\plain {\f0\fs24\b0\i0 Still, the most popular solution towards sIOP is to establish a convention on the semantics of the terms that are used during the collaboration. This convention "resolves" the grounding problem in that it represents the know-how to "decode" the data, i.e., to connect the term with what it stands for in reality. Be aware that the realm of that know-how, denoted here as *background knowledge*, represents a semantic monolith, and it is being demarcated by its use in design time only: any software engineer who is made aware of that background knowledge, through bilateral agreements, domain-specific standards or other conventions, can design and implement measures to apply the data in accordance to what they refer to in reality. This solution is not wrong *per se*, and has as bonus that it does away with the consequences of the unresolved grounding problem because each and every term has already been grounded to everybody's satisfaction. In fact, for long-standing and stable collaborations between software agents, which live in business domains that experience only marginal changes in their business, semantic conventions are the most appropriate approach towards sIOP. Nevertheless, when foreign data emerge that are to be used within the semantic monolith, or when the native data are to be used outside the semantic monolith, sIOP, and hence the IT, will fail. sIOP fails because the convention was not shared to these foreign software agents. Moreover, any run-time attempt to communicate the background knowledge that is reflected by the convention is doomed to fail, because knowledge is just data and thus represented by terms that, in this case, are ungrounded. Again, human intervention is required. This time its purpose is not to resolve the grounding problem, but to bridge the semantic gap that originates from differences in groundings between each pair of collaborating software agents. This is called *semantic reconciliation*, and results in an *alignment* between both groundings. Although weak AI can support in that task [@Euzenat:2013ie], due to its inherent limitations it cannot fulfil the task completely without human intervention. In conclusion, to break the semantic monolith and provide a pair of software agents with sIOP, human intervention is required to produce an alignment between the semantic groundings of each software agent. The issues in this matter relate to (i) the inherent conflict between having a human in the loop and an access-and-play demand; (ii) ***analyse the paragraph below and formulate its issues concretely here*** .}}