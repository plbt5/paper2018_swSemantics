{\rtf1\ansi\ansicpg1252\uc1\deff0
{\fonttbl{\f0\fmodern\fcharset0\fprq2 CourierNewPSMT;}}
{\colortbl;\red0\green0\blue0;\red255\green255\blue255;}
\paperw12240\paperh15840\margl1800\margr1800\margt1440\margb1440\fet2\ftnbj\aenddoc
\pgnrestart\pgnstarts0
\pard\plain \ltrch\loch {\f0\fs24\b0\i0 A full automation of SIOp in order to deliver access-and-play interoperability is impossible: in order to use the data from nation A, nation B needs their genuine understanding in advance. The major impediments to such automation is laid in the nature of semantics itself: The meaning of a term ultimately relates to what it denotes in reality, whilst this relation cannot be deferred from the shape, structure or other characteristics of the term itself due to its total arbitrariness. This semiotic explanation on semantics is depicted in Figure ##, and confronts us with the inevitable separation between languages (software code, modelling languages and natural languages alike) and entities in reality (e.g., things, events, properties of things). That terms and real entities are fundamentally different is hardly a new insight when dealing with information systems. However, addressing this fundamental distinction is at best extremely academic [@Steels:2008tr] and without practical solutions at all [@Cregan2007]. To overcome the separation between terms and entities is in artificial intelligence (AI) known as the *grounding problem*. And despite the progress of AI, its capability to gain even a beginning of a genuine understanding, also known as "strong AI", does not yet exist and is expected to emerge on the long term only, if ever [@XiuquanLi2017]. Its counterpart "weak AI" with its otherwise highly relevant and important achievements in reasoning, prediction and analysis, is based on machinery that relies on language only and can therefore never make the step to reality on its own [@Scheider:2012tj]. Negligence of the existence of the grounding problem and its semiotic origins gives rise to two major impediments in information technology, as follows:}
\par\plain {\f0\fs24\b0\i0 * Firstly, we don't understand the characteristics of semantics sufficiently, or in other words, what impact is generated by the grounding problem on the construction of a software agent. If we are asked to point at the semantic parts in a software agent, we can't. While the same question about, e.g., its scalability, will render a lecture about the different principles that are applied and the components that are used to its achievement. Consequently, without clear design principles we are at a loss of how to engineer semantics into software agents, and how to provide for components or artifacts that achieve semantics. }
\par\plain {\f0\fs24\b0\i0 * Secondly, without knowing how to engineer semantics into software, we are lacking the bridgehead within the software agent that connects with the semantic bridge. In other words, we do not even know how a semantic bridge looks like. Is it an integrated version of the data schemata of both agents? Is it an as small as possible *third* scheme that addresses the shared parts of both schemata only, to be connected to each of the other two reduced schemata? Do we leave both schemata intact, and introduce a connection between them instead? Only when we have come to a conclusion on the semantic bridge, we can address subsequent issues that relate to other architectural concerns, such as scalability? }
\par\pard\plain \ltrch\loch \f0\fs24\b0\i0}